{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Statistics\n",
    "\n",
    "G. Richards, 2016\n",
    "\n",
    "Resources for this material include Ivezic Sections 1.2, 3.0, and 3.1, Karen' Leighly's [Bayesian Statistics Lecture](http://seminar.ouml.org/lectures/bayesian-statistics/), and [Jo Bovy's class](http://astro.utoronto.ca/%7Ebovy/teaching.html), specifically Lecture 1.\n",
    "\n",
    "Last time we worked through some examples of the kinds of things that we will be doing later in the course.  But before we can do the fun stuff, we need to lay some statistical groundwork.  Some of you may have encountered some of this material in Math 311."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notation\n",
    "\n",
    "First we need to go over some of the notation that the book uses.   \n",
    "\n",
    "$x$ is a scalar quantity, measured $N$ times\n",
    "\n",
    "$x_i$ is a single measurement with $i=1,...,N$\n",
    "\n",
    "$\\{x_i\\}$ refers to the set of all N measurements\n",
    "\n",
    "We are generally trying to *estimate* $h(x)$, the ***true*** distribution from which the values of $x$ are drawn. We will refer to $h(x)$ as the probability density (distribution) function or the \"**pdf**\" and $h(x)dx$ is the propobability of a value lying between $x$ and $x+dx$. A histogram is an example of a pdf.\n",
    "\n",
    "While $h(x)$ is the \"true\" distribution (or **population** pdf), what we *measure* from the data is the ***empirical*** distribution, which is denoted $f(x)$.  So, $f(x)$ is a *model* of $h(x)$.  In principle, with infinite data $f(x) \\rightarrow h(x)$, but in reality measurement errors keep this from being strictly true.\n",
    "\n",
    "If we are attempting to guess a *model* for $h(x)$, then the process is *parametric*.  With a model solution we can generate new (simulated) data that should mimic what we measure.  \n",
    "\n",
    "If we are not attempting to guess a model, then the process is *nonparametic*.  That is we are just trying to describe the data that we see in the most compact manner that we can, but we are not trying to produce mock data.\n",
    "\n",
    "The histograms that we made last time are an example of a nonparametric method of describing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "We could summarize the goal of the first few weeks of this class as an attempt to: \n",
    "1. estimate $f(x)$ from some real (possibly multi-dimensional) data set, \n",
    "2. find a way to describe $f(x)$ and its uncertainty, \n",
    "3. compare it to models of $h(x)$, and then \n",
    "4. use the knowledge that we have gained in order to interpret new measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability\n",
    "\n",
    "The probability of $A$, $p(A)$, is the probability that some event will happen (say a coin toss coming up tails), or if the process is continuous, the probability of $A$ falling in a certain range.  (N.B., Technically these two things are different and sometimes are indicated by $P$ and $p$, but I'm ignoring that here.)  $p(A)$ must be positive definite for all $A$ and the sum/integral of the pdf must be unity.\n",
    "\n",
    "If we have two events, $A$ and $B$, the possible combinations are illustrated by the following figure:\n",
    "![Figure 3.1](http://www.astroml.org/_images/fig_prob_sum_1.png)\n",
    "\n",
    "$A \\cup B$ is the *union* of sets $A$ and $B$.\n",
    "\n",
    "$A \\cap B$ is the *intersection* of sets $A$ and $B$.\n",
    "\n",
    "The probability that *either* $A$ or $B$ will happen (which could include both) is the *union*, given by\n",
    "\n",
    "$$p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$$\n",
    "\n",
    "The figure makes it clear why the last term is necessary.  Since $A$ and $B$ overlap, we are double-counting the region where *both* $A$ and $B$ happen, so we have to subtract this out.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The probability that *both* $A$ and $B$ will happen, $p(A \\cap B)$, can be written as\n",
    "$$p(A \\cap B) = p(A|B)p(B) = p(B|A)p(A)$$\n",
    "\n",
    "where p(A|B) is the probability of A *given that* B is true and is called the *conditional probability*.  So the $|$ is short for \"given that\".\n",
    "\n",
    "The *law of total probability* says that\n",
    "\n",
    "$$p(A) = \\sum_ip(A|B_i)p(B_i)$$\n",
    "\n",
    "Example:\n",
    "\n",
    "    A = hit head on door frame, B = { is tall, is average, is short }\n",
    "    P(A) = P(A|is tall) + P(A|is average) + P(A|is short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "N.B.  Just to be annoying, different people use different notation and the following all mean the same thing\n",
    "$$p(A \\cap B) = p(A,B) = p(AB) = p(A \\,{\\rm and}\\, B)$$\n",
    "\n",
    "I'll use the comma notation as that is what the book uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is important to realize that the following is *always* true\n",
    "$$p(A,B) = p(A|B)p(B) = p(B|A)p(A)$$\n",
    "\n",
    "However, if $A$ and $B$ are independent, then \n",
    "\n",
    "$$p(A,B) = p(A)p(B)$$\n",
    "\n",
    "Example:\n",
    "\n",
    "     John is successful and John is a Libra.\n",
    "     \n",
    "In other words, knowing A happened tells us nothing about whether B happened (or will happen), and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look an example.\n",
    "\n",
    "If you have a bag with 5 marbles, 3 yellow and 2 blue and you want to know the probability of picking 2 yellow marbles in a row, that would be\n",
    "\n",
    "$$p(Y_1,Y_2) = p(Y_2|Y_1)p(Y_1).$$\n",
    "\n",
    "$p(Y_1) = \\frac{3}{5}$ since you have an equally likely chance of drawing any of the 5 marbles.\n",
    "\n",
    "If you did not put the first marble back in the back after drawing it (sampling *without* \"replacement\"), then the probability\n",
    "$p(Y_2|Y_1) = \\frac{2}{4}$, so that\n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{2}{4} = \\frac{3}{10}.$$\n",
    "\n",
    "But if you put the first marble back, then\n",
    "$p(Y_2|Y_1) = \\frac{3}{5} = p(Y_2)$, so that \n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{3}{5} = \\frac{9}{25}.$$\n",
    "\n",
    "In the first case $A$ and $B$ (or rather $Y_1$ and $Y_2$) are *not* independent, whereas in the second case they are.\n",
    "\n",
    "We say that two random variables, $A$ and $B$ are independent *iff*\n",
    "$p(A,B) = p(A)p(B)$\n",
    "such that knowing $B$ does not give any information about $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more complicated example from Jo Bovy's class at UToronto\n",
    "![Bovy_L1-StatMiniCourse_page21](figures/Bovy_L1-StatMiniCourse_page21.png)\n",
    "\n",
    "So\n",
    "$$p(A \\,{\\rm or}\\, B|C) = p(A|C) + p(B|C) - p(A \\, {\\rm and}\\, B|C)$$\n",
    "\n",
    "We could get more complicated than that, but let's leave it there for now as this is all that we need right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Need more help with this?  Try watching some Khan Academy videos and working through the exercises:\n",
    "[https://www.khanacademy.org/math/probability/probability-geometry](https://www.khanacademy.org/math/probability/probability-geometry)\n",
    "\n",
    "[https://www.khanacademy.org/math/precalculus/prob-comb](https://www.khanacademy.org/math/precalculus/prob-comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "We have that \n",
    "$$p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "\n",
    "We can define the ***marginal probability*** as\n",
    "$$p(x) = \\int p(x,y)dy,$$\n",
    "where marginal means essentially projecting on to one axis (integrating over the other axis).\n",
    "\n",
    "We can re-write this as\n",
    "$$p(x) = \\int p(x|y)p(y) dy$$\n",
    "\n",
    "This is just the law of total probability (as defined above), but for continous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An illustration might help.  In the following figure, we have a 2-D distribution in $x-y$ parameter space.  Here $x$ and $y$ are *not* independent as, once you pick a $y$, your values of $x$ are constrained.\n",
    "\n",
    "The *marginal* distributions are shown on the left and bottom sides of the left panel.  As the equation above says, this is just the integral along the $x$ direction for a given $y$ (left side panel) or the integral along the $y$ direction for a given $x$ (bottom panel).  \n",
    "\n",
    "The three panels on the right show the *conditional* probability (of $x$) for three $y$ values: $p(x|y=y_0)$.  These are just \"slices\" through the 2-D distribution.\n",
    "\n",
    "![http://www.astroml.org/_images/fig_conditional_probability_1.png](http://www.astroml.org/_images/fig_conditional_probability_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Since $p(x|y)p(y) = p(y|x)p(x)$ we can write that\n",
    "$$p(y|x) = \\frac{p(x|y)p(y)}{p(x)} = \\frac{p(x|y)p(y)}{\\int p(x|y)p(y) dy}$$\n",
    "which in words says that\n",
    "\n",
    "> the (conditional) probability of $y$ given $x$ is just the (conditional) probability of $x$ given $y$ times the (marginal) probability of $y$ divided by the (marginal) probability of $x$, where the latter is just the integral of the numerator.\n",
    "\n",
    "This is **Bayes' rule**, which itself is not at all controverial, though its application can be as we'll discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Lego's \n",
    "\n",
    "An example with Lego's (it's awesome):\n",
    "[https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Monty Hall Problem\n",
    "\n",
    "You are playing a game show and are shown 2 doors.  One has a car behind it, the other a goat.  What are your chances of picking the door with the car?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, now there are 3 doors: one with a car, two with goats.  The game show host asks you to pick a door, but not to open it yet.  Then the host opens one of the other two doors (that you did not pick), making sure to select one with a goat.  The host offers you the opportunity to switch doors.  Do you?\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/180px-Monty_open_door.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/180px-Monty_open_door.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you are back at the 2 door situation.  But what can you make of your prior information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$p(1{\\rm st \\; choice}) = 1/3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$p({\\rm other}) = 2/3$\n",
    "which doesn't change after host opens door without the prize.\n",
    "So, switching doubles your chances.  But only because you had prior information.  If someone walked in after the \"bad\" door was opened, then their probability of winning is the expected $1/2$.\n",
    "\n",
    "Try it:\n",
    "https://betterexplained.com/articles/understanding-the-monty-hall-problem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This example is easier to understand if you do the same thing, but start with a much larger number of doors.\n",
    "\n",
    "For $N$ choices, revealing $N-2$ \"answers\" doesn't change the probability of your choice.  It is still $\\frac{1}{N}$.  But it *does* change the probability of your knowledge of the *other* remaining choice by $N-1$ and it is $\\frac{N-1}{N}$.\n",
    "\n",
    "This is an example of the use of *conditional* probability, where we have $p(A|B) \\ne p(A)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Contingency Table\n",
    "\n",
    "We can also use Bayes' rule to learn something about false positives and false negatives.\n",
    "\n",
    "Let's say that we have a test for a disease.  The test can be positive ($T=1$) or negative ($T=0$) and one can either have the disease ($D=1$) or not ($D=0$).  So, there are 4 possible combinations:\n",
    "$$T=0; D=0 \\;\\;\\;  {\\rm true \\; negative}$$\n",
    "$$T=0; D=1 \\;\\;\\; {\\rm false \\; negative}$$\n",
    "$$T=1; D=0 \\;\\;\\; {\\rm false \\; positive}$$\n",
    "$$T=1; D=1 \\;\\;\\; {\\rm true \\; positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All else being equal, you have a 50% chance of being misdiagnosed.  Not good!  But the probability of disease and the accuracy of the test presumably are not random.\n",
    "\n",
    "If the rates of false positive and false negative are:\n",
    "$$p(T=1|D=0) = \\epsilon_{\\rm FP}$$\n",
    "$$p(T=0|D=1) = \\epsilon_{\\rm FN}$$\n",
    "\n",
    "then the true positive and true negative rates are just:\n",
    "$$p(T=0| D=0) = 1-\\epsilon_{\\rm FP}$$\n",
    "$$p(T=1| D=1) = 1-\\epsilon_{\\rm FN}$$\n",
    "\n",
    "Let's assume that $\\epsilon_{\\rm FP}=0.02$ and $\\epsilon_{\\rm FN}=0.001$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In graphical form this is:\n",
    "![http://www.astroml.org/_images/fig_contingency_table_1.png](http://www.astroml.org/_images/fig_contingency_table_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we have a **prior** regarding how likely the disease is, we can take this into account.\n",
    "\n",
    "$$p(D=1)=\\epsilon_D$$\n",
    "\n",
    "and then $p(D=0)=1-\\epsilon_D$. Say, $\\epsilon_D = 0.01$. \n",
    "\n",
    "Now assume that a person tested positive. What is the probability that this person has the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can't just read $p(D=1|T=1)$ off the table.  That's because the table entry is the conditional probability of the *test* given the *data*, $p(T=1|D=1)$, what we want is the conditional probability of the *data* given the *test*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayes' rule then can be used to help us determine how likely it is that you have the disease if you tested positive:\n",
    "\n",
    "$$p(D=1|T=1) = \\frac{p(T=1|D=1)p(D=1)}{p(T=1)},$$\n",
    "\n",
    "where $$p(T=1) = p(T=1|D=0)p(D=0) + p(T=1|D=1)p(D=1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So\n",
    "$$p(D=1|T=1) = \\frac{(1 - \\epsilon_{FN})\\epsilon_D}{\\epsilon_{FP}(1-\\epsilon_D) + (1-\\epsilon_{FN})\\epsilon_D} \\approx \\frac{\\epsilon_D}{\\epsilon_D+\\epsilon_{FP}}$$\n",
    "\n",
    "That means that to get a reliable diagnosis, we need $\\epsilon_{FP}$ to be quite small.  (Because you *want* the probability to be close to unity if you test positive, otherwise it is a *false* positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example with a disease rate of 1% ($\\epsilon_D = 0.01$) and a false positive rate of 2% ($\\epsilon_{\\rm FP}=0.02$), we have \n",
    "\n",
    "$$p(D=1|T=1) = \\frac{0.01}{0.01+0.02} = 0.333$$\n",
    "\n",
    "Then in a sample of 1000 people, 10 people will *actually* have the disease $(1000*0.01)$, but another 20 $(1000*0.02)$ will test positive, despite being healthy!\n",
    "\n",
    "Therefore, in that sample of 30 people who tested positive, only 1/3 has the disease (not 98% or 99.9% as you might have expected!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Data \n",
    "\n",
    "In this class, we generally won't be dealing with the probability of events $A$ and $B$, rather we will be dealing with models and data, where we are trying to determine the model, given the data.  So, we can rewrite Bayes' rule as\n",
    "$$p({\\rm model}|{\\rm data}) = \\frac{p(\\rm{data}|\\rm{model})p(\\rm{model})}{p(\\rm{data})}.$$\n",
    "\n",
    "We can write this in words as:\n",
    "$${\\rm Posterior Probability} = \\frac{{\\rm Likelihood}\\times{\\rm Prior}}{{\\rm Evidence}},$$\n",
    "\n",
    "where we interpret the posterior probability as the probability of the model (including the model parameters).\n",
    "\n",
    "We'll talk more about models next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "## Distributions\n",
    "\n",
    "Our goal is ultimately to figure out the *distribution* from which our data is drawn, i.e., we want to know the *model*.  For example, let's say that we are trying to characterize the population of asteroids in the Solar System.  Maybe their sizes have a Gaussian distribution (with some characteristic size), or maybe they have a flat distribution (with equal numbers over a large range of sizes).  Or maybe the distribution is a power-law, with lots of little asteroids and very few big ones.  Or maybe it is a power-law in the other direction: very few little ones and lots of big ones.  If you are the first person to discover asteroids, then *you don't know*.  Our job is to figure that out: based entirely on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That leads us to the need for **estimators**.  Since we don't know the distribution, we have to estimate it.  \n",
    "\n",
    "So, the book spends a lot of time talking about estimators and possible distributions.  \n",
    "\n",
    "Let's first review some commonly computed statistical properties of a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell to generate an array with 1000 random numbers\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from astroML import stats as astroMLstats\n",
    "data = np.random.random(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **arithmetic mean** (or Expectation value) is\n",
    "\n",
    "$$\\mu = E(x) = \\int_{-\\infty}^{\\infty} x h(x) dx,$$\n",
    "\n",
    "where $h(x)$ must be properly normalized and the integral gets replaced by a sum for discrete distributions.\n",
    "\n",
    "Specifically, this is the expecation value of $x$.  If you want the expectation value of something else--say $x^2$ or $(x-\\mu)^2$, you replace $x$ with that.\n",
    "\n",
    "We could/should really think about this as the expected location (if the model is a Gaussian, where do you center your Gaussian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747301550336\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell\n",
    "mean = np.mean(data)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While it is perhaps most common to compute the mean, the median is a more *robust* estimator of the (true) mean location of the distribution.  That's because it is less affected by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.51252510321749112, 0.51252510321749112)\n",
      "(0.74730155033553158, 1.2152033416789818)\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell.  Think about what it is doing.\n",
    "median = np.median(data)\n",
    "mask = data>0.75\n",
    "data[mask] = data[mask]*2\n",
    "newmedian = np.median(data)\n",
    "newmean = np.mean(data)\n",
    "print(median,newmedian)\n",
    "print(mean,newmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition to the \"average\", we'd like to know something about **deviations** from the average.  The simplest thing to compute is $$d_i = x_i - \\mu.$$  However, the average deviation is zero by definition of the mean.  The next simplest thing to do is to compute the mean absolute deviation (MAD):\n",
    "$$\\frac{1}{N}\\sum|x_i-\\mu|,$$\n",
    "but the absolute values can hide the true scatter of the distribution [in some cases (see footnote)](http://www.mathsisfun.com/data/standard-deviation.html).  So the next simplest thing to do is to square the differences $$\\sigma^2 = \\frac{1}{N}\\sum(x_i-\\mu)^2,$$ which we call the **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Indeed the *variance* is just expectation value of $(x-\\mu)^2$\n",
    "\n",
    "$$\\sigma^2 = V = \\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n",
    "where, again,  the integral gets replaced by a sum for discrete distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we define the **standard deviation** as\n",
    "$$\\sigma = \\sqrt{V}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.9286273848175111, 1.3887502960638789)\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell\n",
    "var = np.var(data)\n",
    "std = np.std(data)\n",
    "print(var,std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is also the Median Absolute Deviation (also MAD) given by\n",
    "$${\\rm median} (|x_i-{\\rm median}(\\{x_i\\})|)$$\n",
    "where $\\sigma = 1.4826\\,{\\rm MAD}$ for a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.25103774343060964, 0.3721885584102218)\n"
     ]
    }
   ],
   "source": [
    "from astropy.stats import median_absolute_deviation\n",
    "MAD = median_absolute_deviation(data)\n",
    "print(MAD,MAD*1.4826)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Percentiles, $q_p$, are computed as\n",
    "$$\\frac{p}{100} = \\int_{-\\infty}^{q_p}h(x) dx$$\n",
    "\n",
    "For example, the 25th, 50th, and 75th percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.28054080815226445, 0.51252510321749112, 3.0690937142079577)\n"
     ]
    }
   ],
   "source": [
    "q25,q50,q75 = np.percentile(data,[25,50,75])\n",
    "print(q25,q50,q75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where we call the difference between the 25th and 75th percentiles, $q_{75} - q_{25}$, the *interquartile range*.\n",
    "\n",
    "The median and interquartile range are more _robust_ than the mean and standard deviation.  So, one can create a standard-deviation-like measurement (at least for a Gaussian) from the interquartile range as\n",
    "$\\sigma_G = 0.7413(q_{75} - q_{25})$, which we saw last time.  One reason to use this is the same as for the median.  $\\sigma_G$ is a more *robust* estimator of the scale of the distribution.  The normalization makes it *unbiased* for a perfect Gaussian (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0671573624692074"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this cell\n",
    "astroMLstats.sigmaG(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The mode is the most probable value, determined from the peak of the distribution, which is the value where the derivative is 0:\n",
    "$$ \\left(\\frac{dh(x)}{dx}\\right)_{x_m} = 0$$\n",
    "\n",
    "Another way to estimate the mode (at least for a Gaussian distribution) is\n",
    "$$x_m = 3q_{50} - 2\\mu$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModeResult(mode=array([ 0.00065487]), count=array([1]))\n",
      "0.0429722089814\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell  (note that data is not Gaussian so these are very different!)\n",
    "mode = scipy.stats.mode(data)\n",
    "modealt = 3*q50 - 2*mean\n",
    "print(mode)\n",
    "print(modealt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other useful measures include the \"higher order\" moments (the skewness and kurtosis):\n",
    "\n",
    "$$\\Sigma = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^3 h(x) dx,$$\n",
    "\n",
    "$$K = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^4 h(x) dx  - 3.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.2312629664979085, -0.31786323940935324)\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell\n",
    "skew = scipy.stats.skew(data)\n",
    "kurt = scipy.stats.kurtosis(data)\n",
    "print(skew,kurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4877873273039302, 0.4867976044615356, 1.8066676463639546, 1.3441233746810426, 1.2312629664979085, -0.31786323940935324, array([0.00105872]), 0.4848181587767463, 0.23968020535076665, 0.4867976044615356, 0.7266678529429992)\n"
     ]
    }
   ],
   "source": [
    "# Excute this cell\n",
    "print(mean, median, var, std, skew, kurt, mode.mode, modealt, q25, q50, q75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could do the same with a normal distribution with a pdf given by:\n",
    " $$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and Execute this cell\n",
    "# loc = mean (mu)\n",
    "# scale = stddev (sigma)\n",
    "ndata = np.random.normal(loc=0,scale=1,size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00998318235158 -0.0156427975484 1.00464588175 1.00232024909\n",
      "-0.00666167879791 0.0202729955634 [-3.76261768]\n",
      "[-0.68188203 -0.0156428   0.67613463]\n"
     ]
    }
   ],
   "source": [
    "# Compute all the above stats for this distribution\n",
    "print np.mean(ndata), np.median(ndata), np.var(ndata), np.std(ndata)\n",
    "print scipy.stats.skew(ndata), scipy.stats.kurtosis(ndata), scipy.stats.mode(ndata).mode\n",
    "print np.percentile(ndata, [25,50,75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample vs. Population Statistics \n",
    "\n",
    "Statistics estimated from the *data* are called _sample statistics_ as compared to _population statistics_ which come from knowing the functional form of the pdf.  Up to now we have been computing population statistics.\n",
    "\n",
    "Specifically, $\\mu$ is the *population average*, i.e., it is the expecation value of $x$ for $h(x)$.  But we don't *know* $h(x)$.  So the **sample mean**, $\\overline{x}$, is an *estimator* of $\\mu$, defined as\n",
    "$$\\overline{x} \\equiv \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which we determine from the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then instead of $\\sigma^2$, which is the population variance, we have the **sample variance**, $s^2$, where\n",
    "\n",
    "$$s^2 = \\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\overline{x})^2$$\n",
    "\n",
    "Where it is $N-1$ instead of $N$ since we had to determine $\\overline{x}$ from the data instead of using a known $\\mu$.  Ideally one tries to work in a regime where $N$ is large enough that we can be lazy and ignore this. \n",
    "\n",
    "So the mean and variance of a distribution are $\\mu$ and $\\sigma^2$.  The *estimators* of the distribution are $\\overline{x}$ (or $\\hat{x}$) and $s^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias\n",
    "\n",
    "If there is a difference between the *estimator* and the *population* values, we say that the estimator is **biased** (perhaps not quite the usage of the word that you are used to).  E.g., if your distribution is Gaussian and $\\overline{x}$ is a biased estimator of $\\mu$, then the Gaussian is not in the right place.\n",
    "\n",
    "Again, more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty\n",
    "\n",
    "We would also like to know the uncertainty of our estimates $\\overline{x}$ and $s$.  Note that $s$ is **NOT** the uncertainty of $\\overline{x}$.  Rather the uncertainty of $\\overline{x}$, $\\sigma_{\\overline{x}}$ is \n",
    "$$ \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{N}},$$\n",
    "which we call the *standard error of the mean*.  So, the accuracy to which we know the mean is smaller than the width of the distribution.\n",
    "\n",
    "The uncertainty of $s$ itself is\n",
    "$$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_{\\overline{x}}.$$\n",
    "\n",
    "Note that for large $N$, $\\sigma_{\\overline{x}} \\sim \\sqrt{2}\\sigma_s$ and for small $N$, $\\sigma_s$ is not much smaller than $s$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
