{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "381656e7-7071-4f30-8d24-5c50925acedd"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "G. Richards (2016,2018), where I found this video series particularly helpful in trying to simplify the explanation https://www.youtube.com/watch?v=bxe2T-V8XRs.  Thanks also to Vince Baker (Drexel).\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are a simplified computation architecture based loosely on the real neural networks found in brains. \n",
    "\n",
    "In the image below the circles on the left represent the **attributes** of our input data, $X$, which here is 3 dimensional.  The circles in the middle represent the neurons.  They take in the information from the input and, based on some criterion decide whether or not to \"fire\".  The collective results of the neurons in the hidden layer produce the output, $y$, which is represented by the circles on the right, which here is 2 dimensional result.  The lines connecting the circles represent the synapses.  This is a simple example with just one layer of neurons; however, there can be many layers of neurons.\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c8a9551b-8231-4274-a9a0-f995a8a6b44a"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In more detail:\n",
    "\n",
    "The job of a synapses is to take input values and multiply them by some weight before passing them to the neuron (hidden layer):\n",
    "\n",
    "$$z = \\sum_i w x_i$$\n",
    "\n",
    "The neuron then sums up the inputs from all of the synapses connected to it and applies an \"activation function\".  For example a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function.\n",
    "\n",
    "$$a = \\frac{1}{1+e^{-z}}.$$\n",
    "\n",
    "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/500px-Logistic-curve.svg.png)\n",
    "\n",
    "What the neural network does is to learn the weights of the synapses that are needed to produce an accurate model of $y_{\\rm train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bc6f4840-b67d-45c4-95e3-b16e574ca650"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than think about the inputs individually, we can write this process in matrix form as\n",
    "$$X W^{(1)} = Z^{(2)}.$$\n",
    "\n",
    "If $D$ is the number of attributes (here 3) and $H$ is the number of neurons in the hidden layer (here 4), then $X$ is an $N\\times D$ matrix, while $W^{(1)}$ is a $D\\times H$ matrix.  The result, $Z^{(2)}$, is then an $N\\times H$ matrix.\n",
    "\n",
    "We then apply the activation function to each entry of $Z^{(2)}$ independently: \n",
    "$$A^{(2)} = f(Z^{(2)}),$$\n",
    "where $A^{(2)}$ is the output of the neurons in the hidden layer and is also $N\\times H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cafa3925-c1d6-43f9-a2ca-df4023cf2909"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These values are then the inputs for the next set of synapses, where we multiply the inputs by another set of weights, $W^{(2)}:$\n",
    "$$A^{(2)} W^{(2)} = Z^{(3)},$$\n",
    "\n",
    "where $W^{(2)}$ is an $H\\times O$ matrix and $Z^{(3)}$ is an $N\\times O$ matrix with $O$-dimensional output.\n",
    "\n",
    "Another activation function is then applied to $Z^{(3)}$ to give\n",
    "$$\\hat{y} = f(Z^{(3)}),$$\n",
    "which is our estimator of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b67de090-530a-4997-b57a-138100035369"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example we might have $N=100$ people for which we have measured \n",
    "* shoe size\n",
    "* belt size\n",
    "* hat size\n",
    "\n",
    "for whom we know their height and weight.  \n",
    "\n",
    "Then we are going to use this to predict the height and weight for people where we only know shoe size, belt size, and hat size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "31662fbe-7216-4291-a3f6-5fa8578b3414"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The neural network then essentially boils down to determining the weights, which are usually initialized randomly.\n",
    "\n",
    "We do that by minimizing the cost function (which compares the true values of $y$ to our predicted values).  Typically:\n",
    "$$ {\\rm Cost} = J = \\sum\\frac{1}{2}(y - \\hat{y})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we just had 1 weight and we wanted to check 1000 possible values, that wouldn't be so bad.  But we have 20 weights, which means checking $20^{1000}$ possible combinations.    Remember the curse of dimensionality?  That might take a while.  Indeed, far, far longer than the age of the Universe.\n",
    "\n",
    "How about just checking 3 points for each weight and see if we can at least figure out which way is \"down hill\"?  That's a start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can rewrite $J$ as\n",
    "$$ J = \\sum\\frac{1}{2}\\left(y - f\\left( f(X W^{(1)}) W^{(2)} \\right) \\right)^2$$\n",
    "\n",
    "and then compute\n",
    "$$\\frac{\\partial J}{\\partial W}$$\n",
    "in order to determine the slope of the cost function for each weight.  This is the **gradient descent** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll want $\\partial J/\\partial W^{(1)}$ and $\\partial J/\\partial W^{(2)}$ separately.  This allows us to [*backpropagate*](https://en.wikipedia.org/wiki/Backpropagation) the error contributions along each neuron and to change the weights where they most need to be changed.  It is like each observations gets a vote on which way is \"down hill\".  We compute the vector sum to decide the ultimate down hill direction.\n",
    "\n",
    "Once we know the down hill direction from the derivative, we update the weights by subtracting a scalar times that derivative from the original weights.  That's obviously much faster than randomly sampling all the possible combinations of weights.  Once the weights are set, then you have your Neural Network classifier/regressor.\n",
    "\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn has both [unsupervised Neural Network](http://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#neural-networks-unsupervised) and [supervised Neural Network](http://scikit-learn.org/stable/modules/neural_networks_supervised.html#neural-networks-supervised) examples.  Apparently these are new as Jake VanderPlas didn't know about them.\n",
    "\n",
    "Let's try to use the multi-layer perceptron classifier on the Boston House Price dataset (using 75% of the data for training and 25% for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell after making the test set 25% of the total data set.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "#print boston.DESCR\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=___, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "# Look at the weights\n",
    "print [coef.shape for coef in clf.coefs_]\n",
    "\n",
    "ypred = clf.predict(Xtest)\n",
    "#print ypred, ytest\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(ytest,ypred)\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, that only predicts the value for a fraction of the data set.  Again, we can use Scikit-Learn's [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) to make predictions for the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "yCVpred = cross_val_predict(clf, X, y, cv=10) # Complete\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y,yCVpred)\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recent interest in neural networks surged in 2012 when a team using a deep convolutional neural network acheived record results classifying objects in the [ImageNet](http://image-net.org/) data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is clearly much more sophisticated than our basic perceptron. \"Deep\" networks consist of tens of layers with thousands of neurons. These large networks have become usabel thanks to two breakthroughs: the use of sparse layers and the power of graphics processing units (GPUs).\n",
    "Many image processing tasks involve convolving an image with a 2-dimensional kernel as shown below.\n",
    "![Convolution example](https://developer.apple.com/library/content/documentation/Performance/Conceptual/vImage/Art/kernel_convolution.jpg)\n",
    "The sparse layers or convolutional layers in a deep network contain a large number of hidden nodes but very few synapses. The sparseness arises from the relatively small size of a typical convolution kernel (15x15 is a large kernel), so a hidden node representing one output of the convolution is connected to only a few input nodes. Compare this the our previous perceptron, in which every hidden node was connected to every input node.\n",
    "\n",
    "Even though the total number of connections is greatly reduced in the sparse layers, the total number of nodes and connections in a modern deep network is still enormous. Luckily, training these networks turns out to be a great task for GPU acceleration! Serious work using neural networks is almost always done usign specialized GPU-accelerated platforms."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "a6340146-092b-47d0-8584-e84bb64c0952": {
     "id": "a6340146-092b-47d0-8584-e84bb64c0952",
     "prev": null,
     "regions": {
      "9f3c3dc8-1276-4b96-9b97-e4b352cc7fb5": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0.008349570712902259,
        "y": -0.008482103581361025
       },
       "id": "9f3c3dc8-1276-4b96-9b97-e4b352cc7fb5"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
