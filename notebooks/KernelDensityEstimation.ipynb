{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6\n",
    "\n",
    "Gordon Richards (2016, 2018)\n",
    "\n",
    "In Chapter 6 we will cover the following topics\n",
    "* Non-parametric Density Estimation, specifically Kernel Density Estimation (KDE)\n",
    "* $k$-Nearest Neighbor Density Estimation\n",
    "* Parametric Density Estimation, specifically Gaussian Mixture Models (GMM)\n",
    "* Clustering algorithms, particularly $K$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Density Estimation (KDE)\n",
    "\n",
    "Inferring the pdf of a sample of data is known as *density estimation*.  Essentially we are smoothing the data.\n",
    "\n",
    "Density estimation is useful because identifying low probability regions can help uncover rare sources.  Similarly, if the data can be divided into subsamples, one can estimate the pdf for each subsample and, in turn, determine classifications for new objects.\n",
    "\n",
    "*Nonparametric* density estimation is useful when we know nothing about the underlying distribution of the data since we don't have to specify a model.  This flexibility allows us to capture the shape of the distribution well, at the expense of more difficulty interpreting the results.\n",
    "\n",
    "[*Kernel Density Estimation (KDE)*](https://en.wikipedia.org/wiki/Kernel_density_estimation) is the standard here (and, incidentally, is something that we have been doing in my group for about 15 years now).\n",
    "\n",
    "Let's start by recalling the experiment that we did with 1-D histograms in the first week of class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Modified from Ivezic, Figure 6.1\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Draw the random data\n",
    "np.random.seed(1)\n",
    "x = np.concatenate([np.random.normal(-0.5, 0.3, size=14), np.random.normal(1, 0.3, size=7)])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# First figure: silly histogram binning\n",
    "fig1 = plt.figure(figsize=(8, 4))\n",
    "fig1.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.15, top=0.9, hspace=0.05)\n",
    "\n",
    "FC = '#6666FF'\n",
    "XLIM = (-2, 2.9)\n",
    "YLIM = (-0.09, 1.1)\n",
    "\n",
    "ax = fig1.add_subplot(121)\n",
    "bins = np.linspace(-1.8, 2.7, 13)\n",
    "ax.hist(x, bins=bins, normed=True, histtype='stepfilled', fc='k', alpha=0.3)\n",
    "ax.plot(XLIM, [0, 0], '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "#Shift bin centers by 0.25\n",
    "ax = fig1.add_subplot(122)\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.hist(x, bins=bins + 0.25, normed=True, histtype='stepfilled', fc='k', alpha=0.3)\n",
    "ax.plot(XLIM, [0, 0], '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The underlying distribution in both panels is the same, that is the data points that make up the histogram are the same.  All we have done is shifted the locations of the bins by 0.25.\n",
    "As we saw in Lecture 2, the choice of bin centers can really change the histogram that we make.\n",
    "\n",
    "The next panels are what happens if we center the bins on each point.  This is an example of kernel density estimation using a \"top-hat\" kernel.  It is a good description of the data, but is pretty ugly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig1b = plt.figure(figsize=(8, 4))\n",
    "fig1b.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "ax = fig1b.add_subplot(111)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = (abs(x_plot - x[:, None]) <= 0.5 * binwidth).astype(float)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_ylabel('$p(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can make it look nicer by choosing a different kernel.  That is by choosing a different bin shape.  The next 3 plots show KDEs using Gaussian kernels with different width Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# First figure: transition to KDE\n",
    "fig2 = plt.figure(figsize=(8, 8))\n",
    "fig2.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.0, top=1.0, hspace=0.05)\n",
    "\n",
    "ax = fig2.add_subplot(311)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.1)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "\n",
    "ax = fig2.add_subplot(312)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.7)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, 4 * y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_ylabel('$p(x)$')\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax = fig2.add_subplot(313)\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.2)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This looks better, but gives us a \"Goldilocks\" problem.  The first plot uses a kernel that is too narrow.  The second is too wide.  The third is \"just right\".\n",
    "\n",
    "We can think of KDE as replacing the points with \"clouds\".  Each cloud is described by the kernel $K(u)$, where $K(u)$ can be any function that is smooth, is postive definite, normalizes to unity, has a mean of 0, and has a positive variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A common kernel is the Gaussian kernel that we just used above:\n",
    "\n",
    "$$K(u) = \\frac{1}{(2\\pi)^{D/2}}\\exp^{-u^2/2}$$\n",
    "\n",
    "Note that the \"$D$\" is necessary because while histograms are generally 1-D, the kind of Big Data analysis that we are interested in will be $N$-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once a kernel is chosen the kernel density estimate at a given point, $x$, is given by \n",
    "$$ \\hat{f}(x) = \\frac{1}{Nh^D}\\sum_{i=1}^N K\\left(\\frac{d(x,x_i)}{h}\\right),$$\n",
    "where $\\hat{f}$ is an *estimator* of our distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where does this come from?  Well if you wanted to know the density of points you could compute\n",
    "$\\frac{\\sum_1^N\\delta (x-x_i)}{V},$ where $\\delta (x-x_i)$ is the Delta function, $V$ is the volume, and $N$ is the number of points.  In $D$-dimensional space a volume element is just $h^D$.  Then instead of representing our observation as a delta function, we represent it by our kernel function.  To normalize for the number of points, divide by $N$.\n",
    "\n",
    "The argument of $K$ is just some measure of the distance between $x$ and each $x_i$.  Normally $d(x,x_i) = (x-x_i)$.  For the gaussian kernel that makes $h=\\sigma$.  Take a second to convince yourself that that is the case.  So, you can see how $h$ represents the \"width\" or what is usually called the \"bandwidth\" in this context.\n",
    "\n",
    "You might wonder why the book uses $\\hat{f}$ instead of just $f$ since we already are using $f$ instead of $h$ (the true distribution). I don't know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a comparison of some different possible kernels.  The one that I use most commonly is actually an Epanechnikov kernel since the Gaussian and Exponential have rather long tails.\n",
    "![Ivezic, Figure 6.2](http://www.astroml.org/_images/fig_kernels_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We won't go through the math, but it turns out that the Epanechnikov kernel is \"optimal\" in the sense of minimizing the variance.  That kernel is given by $$K(x) = \\frac{3}{4}(1-x^2),$$\n",
    "for $|x|\\le 1$ and 0 otherwise. Below is the code that produces the plot above.  Add the Epanechnikov kernel to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and Execute\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute Kernels.\n",
    "u = np.linspace(-5, 5, 10000)\n",
    "du = u[1] - u[0]\n",
    "\n",
    "gauss = (1. / np.sqrt(2 * np.pi)) * np.exp(-0.5 * u ** 2)\n",
    "\n",
    "exp = 0.5 * np.exp(-abs(u))\n",
    "\n",
    "tophat = 0.5 * np.ones_like(u)\n",
    "tophat[abs(u) > 1] = 0 # Range of the tophat kernel\n",
    "\n",
    "ep = ____  # Add the Epanechnikov kernel function\n",
    "ep[____]=0 # Set the range of the kernel\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the kernels\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(u, gauss, '-', c='black', lw=3, label='Gaussian')\n",
    "ax.plot(u, exp, '-', c='#666666', lw=2, label='Exponential')\n",
    "ax.plot(u, tophat, '-', c='#999999', lw=1, label='Top-hat')\n",
    "ax.plot(__,__,__,__,label='Epanechnikov')  # Add the Epanechnikov kernel to the plot\n",
    "\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('$u$')\n",
    "ax.set_ylabel('$K(u)$')\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(0, 0.8001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The crucial part of KDE is to determine the optimal value for the width of the kernel.  When we first discussed histograms and KDE we talked about theoretical computation of optimal bandwidths.  Let's now look at how we can empirically determine the optimal bandwidth through [**cross validation**](https://en.wikipedia.org/wiki/Cross-validation_(statistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross validation is related to the construction of training and test sets that we talked about last time.  There are a number of different ways to do this.  For example, you could *randomly sample* to decide which data goes into the training or test sets:\n",
    "![Random Sample Cross Validation Example; Remesan Figure 3.7](http://i.stack.imgur.com/4Lrff.png)\n",
    "\n",
    "Where we aren't just doing this once, but rather many times so that each data point is treated both as a training point and as a test point.\n",
    "\n",
    "We could do this in a more ordered way (e.g., to make sure that each point gets sampled as training/test the same number of times) and do a $K$-fold cross validation.  Here $K$ is the number of \"experiments\" that need to be done so that each data point appears in a test sample once.\n",
    "\n",
    "![K-Fold Cross Validation Example; Remesan Figure 3.8](http://i.stack.imgur.com/fhMza.png)\n",
    "\n",
    "We can take that to the extreme by having $K\\equiv N$, so that in each experiment we leave out just one object.  This is called \"Leave-One-Out\" cross validation:\n",
    "\n",
    "![Leave-One-Out Cross Validation Example; Remesan Figure 3.9](http://images.slideplayer.com/16/4977882/slides/slide_35.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can implement this in Scikit-Learn with [`GridSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html) and replot our histogram above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and Execute this cell to determine the bandwidth\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "bwrange = np.linspace(0.1, 1.0, 30) # Test 30 bandwidths from 0.1 to 1.0\n",
    "K = 5 # 5-fold cross validation\n",
    "grid = GridSearchCV(KernelDensity(), {'bandwidth': bwrange}, cv=K) \n",
    "grid.fit(x[:, None]) #Fit the histogram data that we started the lecture with.\n",
    "h_opt = grid.best_params_['bandwidth']\n",
    "print h_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell to see the new \"histogram\"\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "ax = fig2.add_subplot(111)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], h_opt)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2-D Histograms\n",
    "\n",
    "Here is some sample code using [`sklearn.neighbors.KernelDensity`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html).  Play around with this and see how it works.  Make different variations of the plot.  What we are doing here is using KDE to set the plot color to indicate the relative density of the points.  This is essentially a 2-D histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Two 2-D normal distributions with offset centroids\n",
    "X = np.concatenate([np.random.normal([-1,-1],[0.75,0.75],size=(1000,2)),np.random.normal([1,1],[1,1],size=(500,2))]) \n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
    "kde.fit(X) #fit the model to the data\n",
    "u = v = np.linspace(-4,5,80)\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(u, v))).T\n",
    "dens = np.exp(kde.score_samples(Xgrid)) #evaluate the model on the grid\n",
    "\n",
    "plt.scatter(Xgrid[:,0],Xgrid[:,1], c=dens, cmap=\"Purples\", edgecolor=\"None\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now copy the example from above to a new cell and splice in the cross validation code to produce a new plot with the \"optimal\" bandwidth.  Try `bandwidth=0.01` to `bandwidth=1.0`.  Basically, splice in the lines of code for `GridSearchCV` between the lines setting `X` and instantiating `kde`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
